{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3path = 's3a://upgrad-spark-data/Parking_Violations_Issued_-_Fiscal_Year_2017.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_2017 = spark.read.format(\"csv\").option(\"header\", \"true\").load(s3path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_2017 = tickets_2017.select(\"Summons Number\",\"Plate ID\",\"Registration State\",\"Issue Date\",\"Violation Code\",\"Vehicle Body Type\",\"Vehicle Make\",\"Violation Precinct\",\"Issuer Precinct\",\"Violation Time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_2017.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_2017.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_2017.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Find the total number of tickets for the year.\n",
    "tickets_2017.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For using SQL, you need to create a temporary view\n",
    "tickets_2017.createOrReplaceTempView(\"data_2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Find out the number of unique states from where the cars \n",
    "# that got parking tickets came from. (Hint: Use 'Registration State')\n",
    "spark.sql(\"SELECT count(distinct `Registration State`) as count FROM data_2017\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arranging the dataframe based on number of entries\n",
    "spark.sql(\"SELECT `Registration State`, count(*) as count \\\n",
    "        FROM data_2017 group by `Registration State` order by `count` desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing '99' by 'NY' in the dataframe\n",
    "from pyspark.sql import functions as F\n",
    "tickets_2017_new = tickets_2017.withColumn('Registration State',F.when(tickets_2017['Registration State']=='99','NY').otherwise(tickets_2017['Registration State']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The temporary view needs to be recreated as values have been updated in tickets_2017_new\n",
    "tickets_2017_new.createOrReplaceTempView(\"data_2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT `Registration State`, count(*) as count \\\n",
    "        FROM data_2017 group by `Registration State` order by `count` desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT count(distinct `Registration State`) as count \\\n",
    "                 FROM data_2017').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGGREGATION TASKS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. How often does each violation code occur? (frequency of violation codes - find the top 5)\n",
    "\n",
    "Since we haven't made in changes in the dataframe or the temporary view,we can use the same for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT `Violation Code`, count(*) as count \\\n",
    "                    FROM data_2017 group by `Violation Code` order by `count` desc limit 5').show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. How often does each vehicle body type get a parking ticket?\n",
    "How about the vehicle make? (find the top 5 for both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here, `Violation Code` will be replaced by the each of the following variables:\n",
    "spark.sql('SELECT `Vehicle Body Type`, count(*) as count FROM data_2017 \\\n",
    "                    group by `Vehicle Body Type` order by `count` desc limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## `Vehicle Make`\n",
    "spark.sql('SELECT `Vehicle Make`, count(*) as count FROM data_2017 \\\n",
    "                    group by `Vehicle Make` order by `count` desc limit 5').show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. A precinct is a police station that has a certain zone of the city under its command.\n",
    "Find the (5 highest) frequencies of:\n",
    "\n",
    "3.1 Violating Precincts (this is the precinct of the zone where the violation occurred).\n",
    "Using this, can you make any insights for parking violations in any specific areas of the city?\n",
    "\n",
    "3.2 Issuing Precincts (this is the precinct that issued the ticket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here, `Violation Code` will be replaced by 'Violation Precinct' and 'Issuer Precinct'\n",
    "spark.sql('SELECT `Violation Precinct`, count(*) as count FROM data_2017 \\\n",
    "                    group by `Violation Precinct` order by `count` desc limit 6').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT `Issuer Precinct`, count(*) as count FROM data_2017\\\n",
    "                    group by `Issuer Precinct` order by `count` desc limit 6').show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Find the violation code frequency across 3 precincts which have issued the most number of tickets - do these precinct zones have an exceptionally high frequency of certain violation codes? Are these codes common across precincts?\n",
    "The top 3 precincts which have issued maximum tickets are '19', '14' and '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT `Issuer Precinct`, `Violation Code`, count(*) as count_tickets \\\n",
    "                    FROM data_2017 where `Issuer Precinct` = '19'\\\n",
    "                    group by `Issuer Precinct`, `Violation Code` \\\n",
    "                    order by `count_tickets` desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT `Issuer Precinct`, `Violation Code`, count(*) as count_tickets \\\n",
    "                    FROM data_2017 where `Issuer Precinct` = '14'\\\n",
    "                    group by `Issuer Precinct`, `Violation Code` \\\n",
    "                    order by `count_tickets` desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT `Issuer Precinct`, `Violation Code`, count(*) as count_tickets \\\n",
    "                    FROM data_2017 where `Issuer Precinct` = '1'\\\n",
    "                    group by `Issuer Precinct`, `Violation Code` \\\n",
    "                    order by `count_tickets` desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From the results obtained after the above query, you can see that the violation code '14'is common in all the three precincts."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5. Youd want to find out the properties of parking violations across different times of the day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for missing values\n",
    "spark.sql(\"select count(*) as count\\\n",
    "                 FROM data_2017 where `Violation Time` is Null\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Since there are no null values, you can move forward with the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_2017_new.select('Violation Time').dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the operation to be performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select `Violation Time`, if(right(`Violation Time`, 1) == 'A' or left(`Violation Time`, 2) == '12',concat(substring(`Violation Time`, 1,2),\\\n",
    "                    ':', substring(`Violation Time`, 3,2)), concat(int(substring(`Violation Time`, 1,2) + 12),\\\n",
    "                    ':', substring(`Violation Time`, 3,2))) as `Violation Time 2`\\\n",
    "                    from data_2017 limit 50\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Divide 24 hours into 6 equal discrete bins of time. The intervals you choose are at your discretion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a separate df with the required fields for analysis\n",
    "time_violation_analysis = spark.sql(\"select if(right(`Violation Time`, 1) == 'A' or left(`Violation Time`, 2) == '12',\\\n",
    "  concat(substring(`Violation Time`, 1,2),':', substring(`Violation Time`,3,2)),\\\n",
    "  concat(int(substring(`Violation Time`, 1,2) + 12),':', substring(`Violation Time`, 3,2)))\\\n",
    "  as `Violation Time`, `Violation Code` from data_2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_violation_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For using SQL, you need to create a temporary view\n",
    "time_violation_analysis.createOrReplaceTempView('time_violation_data')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Separating the time into 6 equal bins based on time. Replacing the dataframe with a new dataframe containing all the columns. For each of these groups, find the 3 most commonly occurring violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_violation_analysis = spark.sql('''select case\n",
    "                                       when int(substring(`Violation Time`,1,2)) between 00 and 03\n",
    "                                       then '00:00-03:59'\n",
    "                                       when int(substring(`Violation Time`,1,2)) between 04 and 07\n",
    "                                       then '04:00-07:59'\n",
    "                                       when int(substring(`Violation Time`,1,2)) between 08 and 11\n",
    "                                       then '08:00-11:59'\n",
    "                                       when int(substring(`Violation Time`,1,2)) between 12 and 15\n",
    "                                       then '12:00-15:59'\n",
    "                                       when int(substring(`Violation Time`,1,2)) between 16 and 19\n",
    "                                       then '16:00-19:59'\n",
    "                                       else '20:00-23:59'\n",
    "                                       end as bins,  `Violation Time`, `Violation Code`\n",
    "                                       from time_violation_data''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_violation_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the SQL view\n",
    "time_violation_analysis.createOrReplaceTempView('time_violation_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT bins, `Violation Code`, count(*) as `count`\\\n",
    "                    FROM time_violation_data where bins = '00:00-03:59'\\\n",
    "                    group by bins, `Violation Code`\\\n",
    "                    order by `count` desc limit 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT bins, `Violation Code`, count(*) as `count`\\\n",
    "                    FROM time_violation_data where bins = '04:00-07:59'\\\n",
    "                    group by bins, `Violation Code`\\\n",
    "                    order by `count` desc limit 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT bins, `Violation Code`, count(*) as `count`\\\n",
    "                    FROM time_violation_data where bins = '08:00-11:59'\\\n",
    "                    group by bins, `Violation Code`\\\n",
    "                    order by `count` desc limit 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT bins, `Violation Code`, count(*) as `count`\\\n",
    "                    FROM time_violation_data where bins = '12:00-15:59'\\\n",
    "                    group by bins, `Violation Code`\\\n",
    "                    order by `count` desc limit 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT bins, `Violation Code`, count(*) as `count`\\\n",
    "                    FROM time_violation_data where bins = '16:00-19:59'\\\n",
    "                    group by bins, `Violation Code`\\\n",
    "                    order by `count` desc limit 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT bins, `Violation Code`, count(*) as `count`\\\n",
    "                    FROM time_violation_data where bins = '20:00-23:59'\\\n",
    "                    group by bins, `Violation Code`\\\n",
    "                    order by `count` desc limit 3\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, try another direction. For the 3 most commonly occurring violation codes, find the most common times of day (in terms of the bins from the previous part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the 3 most commonly occurring violation codes\n",
    "spark.sql(\"SELECT `Violation Code`, count(*) as `count`\\\n",
    "                    FROM time_violation_data\\\n",
    "                    group by `Violation Code`\\\n",
    "                    order by `count` desc limit 3\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Violation codes: 21, 36, 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT `Violation Code`, bins, count(*) as `count`\\\n",
    "                    FROM time_violation_data where `Violation Code` = '21'\\\n",
    "                    group by `Violation Code`, bins\\\n",
    "                    order by `count` desc limit 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT `Violation Code`, bins, count(*) as `count`\\\n",
    "                    FROM time_violation_data where `Violation Code` = '36'\\\n",
    "                    group by `Violation Code`, bins\\\n",
    "                    order by `count` desc limit 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT `Violation Code`, bins, count(*) as `count`\\\n",
    "                    FROM time_violation_data where `Violation Code` = '38'\\\n",
    "                    group by `Violation Code`, bins\\\n",
    "                    order by `count` desc limit 1\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For code 21, maximum tickets occur during the time - 08:00 AM to 12:00 PM\n",
    "For code 36, maximum tickets occur during the time - 08:00 AM to 12:00 PM\n",
    "For code 38, maximum tickets occur during the time - 12:00 PM to 04:00 PM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6. Seasonality\n",
    "This can be done in different ways\n",
    "Seasonality can be defined month-wise, or season-wise (3 months)\n",
    "We have performed based on the seasons - Summer, Winter, Spring and Autumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_seasonality = spark.sql('''select `Violation Code`, `Issue Date`, case\n",
    "                                  when month(to_date(`Issue Date`, 'yyyy-MM-dd')) between 03 and 05\n",
    "                                  then 'spring'\n",
    "                                  when month(to_date(`Issue Date`, 'yyyy-MM-dd')) between 06 and 08\n",
    "                                  then 'summer'\n",
    "                                  when month(to_date(`Issue Date`, 'yyyy-MM-dd')) between 09 and 11\n",
    "                                  then 'autumn'\n",
    "                                  when month(to_date(`Issue Date`, 'yyyy-MM-dd')) in (1,2,12)\n",
    "                                  then 'winter'\n",
    "                                  else 'unknown'\n",
    "                                  end as season\n",
    "                                  from data_2017''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_seasonality.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For using SQL, you need to create a temporary view\n",
    "tickets_seasonality.createOrReplaceTempView('seasonal_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select `season`, count(*) as no_of_tickets\\\n",
    "                    from seasonal_data\\\n",
    "                    group by `season`\\\n",
    "                    order by no_of_tickets desc\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The 3 most common violations for each season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select `season`, count(*) as no_of_tickets\\\n",
    "                    from seasonal_data\\\n",
    "                    group by `season`\\\n",
    "                    order by no_of_tickets desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select `season`, `Violation Code`, count(*) as no_of_tickets\\\n",
    "                    from seasonal_data where `season` = 'autumn' \\\n",
    "                    group by season, `Violation Code` order by no_of_tickets desc\\\n",
    "                    limit 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select `season`, `Violation Code`, count(*) as no_of_tickets\\\n",
    "                    from seasonal_data where `season` = 'summer' \\\n",
    "                    group by season, `Violation Code` order by no_of_tickets desc\\\n",
    "                    limit 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select `season`, `Violation Code`, count(*) as no_of_tickets\\\n",
    "                    from seasonal_data where `season` = 'winter' \\\n",
    "                    group by season, `Violation Code` order by no_of_tickets desc\\\n",
    "                    limit 3\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The fines collected from all the parking violation constitute a revenue sourcefor the NYC police department. Let's take an example of estimating that for the 3 most commonly occurring codes.\n",
    "\n",
    "Three most occuring violation codes : 21, 36, 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Total occurrences of the 3 most common violation codes\n",
    "spark.sql(\"select `Violation Code`, count(*) as `no_of_tickets`\\\n",
    "                    from data_2017\\\n",
    "                    group by `Violation Code`\\\n",
    "                    order by `no_of_tickets` desc\\\n",
    "                    limit 3\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Fine amount for each code can be checked from the website\n",
    "http://www1.nyc.gov/site/finance/vehicles/services-violation-codes.page\n",
    "\n",
    "21\t\n",
    "Street Cleaning: No parking where parking is not allowed by sign, \n",
    "street marking or traffic control device\n",
    "Fine amount - average - $55\n",
    "\n",
    "36\n",
    "Exceeding the posted speed limit in or near a designated school zone.\n",
    "Fine amount - average - $50\n",
    "\n",
    "38\n",
    "Parking Meter - Failing to show a receipt or tag in the windshield.\n",
    "Drivers get a 5-minute grace period past the expired time on parking meter receipts.\n",
    "Fine amount - average - $50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''select `Violation Code`, case\n",
    "                    when `Violation Code` = 21\n",
    "                    then 55 * count(*)\n",
    "                    when `Violation Code` = 36\n",
    "                    then 50* count(*)\n",
    "                    when `Violation Code` = 38\n",
    "                    then 50* count(*)\n",
    "                    else '0'\n",
    "                    end as `fine_amount`\n",
    "                    from data_2017\n",
    "                    group by `Violation Code`\n",
    "                    order by `fine_amount` desc\n",
    "                    limit 3''').show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What can you intuitively infer from these findings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
